{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import shuffle\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn as sk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset & Document Paths\"\"\"\n",
    "train_data_dir = \"D:/1 Project Model/Dataset/Layout/map&other/train\"\n",
    "test_data_dir = \"D:/1 Project Model/Dataset/Layout/map&other/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read and Preprocess dataset\"\"\"\n",
    "def read_train_data(data_dir):\n",
    "    \"\"\"Read training dataset.\"\"\"\n",
    "    datas = []\n",
    "    labels = []\n",
    "    fpaths = []\n",
    "    for fname in os.listdir(data_dir):\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        fpaths.append(fpath)\n",
    "        label = int(fname.split(\"_\")[0])\n",
    "        \n",
    "        # No autoaugmentation\n",
    "        image = Image.open(fpath)\n",
    "        data = np.array(image) / 255.0\n",
    "        datas.append(data)\n",
    "        labels.append(label)\n",
    "\n",
    "    datas = np.array(datas)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(\"shape of training datas: {}\\tshape of labels: {}\".format(datas.shape, labels.shape))\n",
    "    return fpaths, datas, labels\n",
    "\n",
    "def read_test_data(data_dir):\n",
    "    \"\"\"Read testing datset.\"\"\"\n",
    "    datas = []\n",
    "    labels = []\n",
    "    fpaths = []\n",
    "    for fname in os.listdir(data_dir):\n",
    "        fpath = os.path.join(data_dir, fname)\n",
    "        fpaths.append(fpath)\n",
    "        image = Image.open(fpath)\n",
    "        data = np.array(image) / 255.0\n",
    "        label = int(fname.split(\"_\")[0])\n",
    "        datas.append(data)\n",
    "        labels.append(label)\n",
    "\n",
    "    datas = np.array(datas)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(\"shape of testing datas: {}\\tshape of labels: {}\".format(datas.shape, labels.shape))\n",
    "    return fpaths, datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training datas: (650, 256, 256, 3)\tshape of labels: (650,)\n",
      "shape of testing datas: (60, 256, 256, 3)\tshape of labels: (60,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load datasets\"\"\"\n",
    "fpaths, datas, labels = read_train_data(train_data_dir)\n",
    "test_paths, test_data, test_label = read_test_data(test_data_dir)\n",
    "\n",
    "# The number of training dataset\n",
    "num_train_examples = datas.shape[0]\n",
    "# The number of testing dataset\n",
    "num_test_examples = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(datas, feature_fns, verbose=True)\n",
    "X_test_feats = extract_features(test_data, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(datas, feature_fns, verbose=True)\n",
    "X_test_feats = extract_features(test_data, feature_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from linear_classifier import LinearSVM\n",
    "\n",
    "learning_rates = 1e-7\n",
    "regularization_strengths = 4000000\n",
    "\n",
    "svm = LinearSVM()\n",
    "loss_hist = svm.train(X_train_feats, labels, learning_rates, regularization_strengths, num_iters=6000)\n",
    "y_train_pred = svm.predict(X_train_feats)\n",
    "train_accuracy = np.mean(labels == y_train_pred)\n",
    "y_test_pred = svm.predict(X_test_feats)\n",
    "test_accuracy = np.mean(test_label== y_test_pred)\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset & Document Paths\"\"\"\n",
    "train_data_dir = \"D:/1 Project Model/Dataset/Platform/train_2000\"\n",
    "test_data_dir = \"D:/1 Project Model/Dataset/Platform//test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training datas: (2000, 256, 256, 3)\tshape of labels: (2000,)\n",
      "shape of testing datas: (100, 256, 256, 3)\tshape of labels: (100,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load datasets\"\"\"\n",
    "fpaths, datas, labels = read_train_data(train_data_dir)\n",
    "test_paths, test_data, test_label = read_test_data(test_data_dir)\n",
    "\n",
    "# The number of training dataset\n",
    "num_train_examples = datas.shape[0]\n",
    "# The number of testing dataset\n",
    "num_test_examples = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(datas, feature_fns, verbose=True)\n",
    "X_test_feats = extract_features(test_data, feature_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确度: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_feats, labels)\n",
    "y_predict = dtc.predict(X_test_feats)\n",
    "print(\"准确度:\", dtc.score(X_test_feats, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
